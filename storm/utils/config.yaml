# Environment name
chaohu:
  train: VDN
  # strategies
  VDN:
    # Arguments for the agent structure
    agent_class: VDN # agent class
    if_mac: True # if is multi-agent
    if_double: True # if use double network
    if_recurrent: False # if use recurrent
    seq_len: 3 # recurrent sequence length
    net_dim: 128 # dimension of hidden layers# dimension of hidden layers
    num_layer: 3 # number of network layers
    if_dueling: True # if use dueling layer

    # Arguments for the training
    max_capacity: 250000 # maximum capacity for the replay buffer
    batch_size: 128 # batch size of the training data
    gamma: 0.95 # discouted rate
    learning_rate: 0.0001 # learning rate
    update_interval: 0.05 # update tau for target network
    repeat_times: 2 # repeatedly update network using ReplayBuffer
    loss_function: MeanSquaredError # loss function
    optimizer: Adam # optimizer

    # Arguments for the exploration
    total_episodes: 5000 # training steps
    pre_episodes: 100 # pre-sampling steps
    ini_episodes: 0 # initial episodes n
    explore_events: 20 # exploration events
    epsilon_decay: 0.999 # epsilon-greedy noise
    epsilon_min: 0.1 # minimum epsilon-greedy noise

    # Arguments for the evaluation
    eval_events: 1 # evaluation events
    eval_gap: 10 # evaluate the agent per eval_gap
    
    # Arguments for the training process
    save_gap: 100 # save the agent & state_norm per save_gap
    cwd: ./model/chaohu_VDN/reward2_2 # the working directory
    if_remove: True # if remove the cwd or keep it
    if_load: False # if load the current model
    replace_rain: False # if replace the rainfall events
    processes: 5 # for parallel sampling, use 0 or 1 to close multiprocess

  DQN:
    # Arguments for the agent structure
    agent_class: DQN # agent class
    if_mac: False # if is multi-agent
    if_double: True # if use double network
    if_recurrent: False # if use recurrent
    seq_len: 3 # recurrent sequence length
    net_dim: 256 # dimension of hidden layers# dimension of hidden layers
    num_layer: 3 # number of network layers
    if_dueling: True # if use dueling layer

    # Arguments for the training
    max_capacity: 250000 # maximum capacity for the replay buffer
    batch_size: 128 # batch size of the training data
    gamma: 0.95 # discouted rate
    learning_rate: 0.0001 # learning rate
    update_interval: 0.05 # update tau for target network
    repeat_times: 2 # repeatedly update network using ReplayBuffer
    loss_function: MeanSquaredError # loss function
    optimizer: Adam # optimizer

    # Arguments for the exploration
    total_episodes: 5000 # training steps
    pre_episodes: 100 # pre-sampling steps
    ini_episodes: 0 # initial episodes n
    explore_events: 20 # exploration events
    epsilon_decay: 0.999 # epsilon-greedy noise
    epsilon_min: 0.1 # minimum epsilon-greedy noise

    # Arguments for the evaluation
    eval_events: 1 # evaluation events
    eval_gap: 10 # evaluate the agent per eval_gap

    
    # Arguments for the training process
    save_gap: 100 # save the agent & state_norm per save_gap
    cwd: ./model/chaohu_DQN/reward # the working directory
    if_remove: True # if remove the cwd or keep it
    if_load: False # if load the current model
    replace_rain: False # if replace the rainfall events
    processes: 5 # for parallel sampling, use 0 or 1 to close multiprocess

  IQL:
    # Arguments for the agent structure
    agent_class: IQL # agent class
    if_mac: True # if is multi-agent
    if_double: True # if use double network
    if_recurrent: False # if use recurrent
    seq_len: 3 # recurrent sequence length
    net_dim: 128 # dimension of hidden layers# dimension of hidden layers
    num_layer: 3 # number of network layers
    if_dueling: True # if use dueling layer

    # Arguments for the training
    max_capacity: 250000 # maximum capacity for the replay buffer
    batch_size: 128 # batch size of the training data
    gamma: 0.95 # discouted rate
    learning_rate: 0.0001 # learning rate
    update_interval: 0.05 # update tau for target network
    repeat_times: 2 # repeatedly update network using ReplayBuffer
    loss_function: MeanSquaredError # loss function
    optimizer: Adam # optimizer

    # Arguments for the exploration
    total_episodes: 5000 # training steps
    pre_episodes: 100 # pre-sampling steps
    ini_episodes: 0 # initial episodes n
    explore_events: 20 # exploration events
    epsilon_decay: 0.999 # epsilon-greedy noise
    epsilon_min: 0.1 # minimum epsilon-greedy noise

    # Arguments for the evaluation
    eval_events: 1 # evaluation events
    eval_gap: 10 # evaluate the agent per eval_gap
    save_gap: 100 # save the agent & state_norm per save_gap
    cwd: ./model/chaohu_IQL/reward # the working directory
    if_remove: True # if remove the cwd or keep it
    if_load: False # if load the current model
    replace_rain: False # if replace the rainfall events
    processes: 5 # for parallel sampling, use 0 or 1 to close multiprocess

  test:
    # Arguments for online search in the testing
    test_events: 100 # testing events
    replace_rain: True # if repalce rain
    test_agents:
      - DQN
      - IQL
      - VDN
    if_predict: False # if use the predictive algorithm
    cwd: results/chaohu # workdir to save the figures & table
    if_remove: False # if remove the results dir
    if_load: False # if load the current result file
    

  # Predictive control paras
  predict:
    algorithm: NSGA2
    pop_size: 32
    sampling: 
      !!python/tuple
        # type
        - Initialize
        # probability
        - 0.4
    crossover:
      !!python/tuple
        # type
        - TwoPointCrossover
        # probability
        # - 1.0
        # # eta
        # - 3.0
    mutation:
      !!python/tuple
        # type
        - PM
        # probability
        - 1.0
        # eta
        - 3.0
    termination:
      !!python/tuple
        - time
        - 00:03:00
        # - n_gen
        # - 5
    seed: 1
    # threads to run the evaluation
    threads: 4

# Environment name
astlingen:
  # strategy
  VDN:
    # Arguments for the agent structure
    agent_class: VDN # agent class
    if_mac: True # if is multi-agent
    if_recurrent: False # if use recurrent
    seq_len: 3 # recurrent sequence length
    net_dim: 128 # dimension of hidden layers# dimension of hidden layers
    num_layer: 3 # number of network layers
    if_dueling: True # if use dueling layer

    # Arguments for the training
    max_capacity: 250000 # maximum capacity for the replay buffer
    batch_size: 128 # batch size of the training data
    gamma: 0.98 # discouted rate
    learning_rate: 0.0001 # learning rate
    update_interval: 0.05 # update tau for target network
    repeat_times: 2 # repeatedly update network using ReplayBuffer
    loss_function: MeanSquaredError # loss function
    optimizer: Adam # optimizer

    # Arguments for the exploration
    total_episodes: 5000 # training steps
    pre_episodes: 2 # pre-sampling steps
    ini_episodes: 0 # initial episodes n
    explore_events: 20 # exploration events
    epsilon_decay: 0.999 # epsilon-greedy noise
    epsilon_min: 0.1 # minimum epsilon-greedy noise

    # Arguments for the evaluation
    eval_events: 1 # evaluation events
    eval_gap: 10 # evaluate the agent per eval_gap
    save_gap: 100 # save the agent & state_norm per save_gap
    cwd: ./model/astlingen_VDN # the working directory
    if_remove: True # if remove the cwd or keep it
    if_load: False # if load the current model
    replace_rain: True # if replace the rainfall events

  MPC:
    # Arguments for online search in the testing
    test_events: 8 # testing events
    if_predict: False # if use the predictive algorithm
    algorithm: NSGA2
    pop_size: 32
    sampling: 
      !!python/tuple
        # type
        - Initialize
        # probability
        - 0.4
    crossover:
      !!python/tuple
        # type
        - SBX
        # probability
        - 1.0
        # eta
        - 3.0
    mutation:
      !!python/tuple
        # type
        - PM
        # probability
        - 1.0
        # eta
        - 3.0
    termination:
      !!python/tuple
        - time
        - 00:03:00
        # - n_gen
        # - 5
    # threads to run the evaluation
    threads: 4